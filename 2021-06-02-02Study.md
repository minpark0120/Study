---
title: 2021-06-02-Thu-Study
categories: [study]
comments: true
---
-------------------------------------------------------------------------------


# 강화학습 환경 만들기

## 출처 https://www.youtube.com/watch?v=PuVLgXhEBpQ

## 강화학습 환경 예제
```
시간이 지나면서 강화학습 환경이 다양해지고 있다.

강화학습 환경 목록: https://github.com/clvrai/awesome-rl-envs

강화학습 환경 한계
    규칙 단순
    할 수 있는 행동의 종류 적다
    결정적이다(Deterministic)
    모든 상태를 관찰할 수 있다.(Fully-observable)

1. Cart-Pole 환경
    나무막대가 쓰러지지 않게 하는 환경

2. Breakout 환경
    아타리게임 일종
    벽돌게임
    Actor-Critic 확인할 때 사용

3. Starcraft 2 환경
    Deepmind + blizzard
    블리자드가 API 제공

4. DOTA

```

## 강화학습 환경 개발이 어려운 이유
```
1. 시뮬레이션 개발
    게임을 직접 만들어야함
    강화학습만 알면되는게 아님

2. 강화학습
    강화학습 개념 숙지
    게임도 개발할줄 알아야함

1,2 의 교집합 = 강화학습 환경 개발
```
## 강화학습 환경 만들 때 주의할 점
### 어떤 환경을 만들지 생각해보기
```
로보틱스
게임
네비게이션
멀티 에이전트
자율주행
```

### MDP 잘 생각해 보기
![MDP](https://user-images.githubusercontent.com/59559270/120480419-f7f8b500-c3e9-11eb-93a4-b3fc1d176711.PNG)
```
게임의 상태를 어떻게 나타낼 것인가
에이전트가 어떤 행동을 할 수 있는가
모든 상태를 관찰할 수 있는가(MDP vs POMDP)
보상을 주기 위해 어떤 정보가 필요한가
```

## 간단한 강화학습 환경 만들기(Code)
```
GridWorld
    도착했을 때만 1
    상하좌우

    openAI Gym 환경 상속

Gym.Env
reset() : 환경 초기화
step() : 환경으로부터 상태 전달 받아 수행 후 환경으로 다시 전달
render() : 환경을 렌더링해 화면으로 출력

gym.make() : 강화학습 환경 불러옴
env.action_space.sample(): 임의의 행동을 선택한다.
```

### 사용할 패키지 가져오기
```py
import gym
import sys
import os
import os
import numpy as np

from gym import spaces

from PIL import Image as Image
GUI 렌더링 위한 matplotlib
import matplotlib.pyplot as plt
```

### 격자에 사용할 색상 정의
![격자를위한색깔](https://user-images.githubusercontent.com/59559270/120480150-b5cf7380-c3e9-11eb-83da-e53f17b51035.PNG)

```
0: space = black
1: 장애물 = gray
2: 도착점 = green
3: 에이전트 = red
4: 에이전트 도착 = blue
```

### 초기화: 행동공간, 관찰 공간 정의
```py
class GridworldEnv(gym.Env): # gym.env 상속
    num_env = 0

    def __init__(self):
        # 초기화함수 구현
        self.actions = [0,1,2,3,4] #가만히 위아래왼쪽오른쪽
        self_inv_actions = [0,2,1,4,3] #반대되는 action 정의
        self.action_space = spaces.Discrete(5): #가능한 action
        self.action_pos_dict = {0: [0,0], 1: [-1,0], 2: [1,0], 3: [0,-1], 4: [0,1]} # action의 포지션

        #observation space
        self.obs_shape = [128, 128, 3] # gridworld의 크기 128*128*3
        self.observation_space = spaces.Box(low=0, high=1, shape =self.obs_shape, dtype-np.float32) #노멛
        #관찰 공간 정의
```

### step함수 구현
```py
    def step(self,action): # 결정된 action 받아옴
        action = init(action)
        next_state = (self.agent_sate[0] + self_action_pos_dict[action][0], self.agent_state[1] + self.action_pos_dict[action][1]) # next state action으로 계산

    if action == 0:
        return (self.observation, 0, False) #action == 0, 2번째인자 reward = 0, 3번째 인자 끝났는가 = False

    if next_state[0] < 0 or next_state[0] >= self.grid_shape[0]:
        return (self.observation, 0, False) #격자 바깥으로 나가지않기
    
    if next_state[1] < 0 or next_state[1] >= self.grid_shape[1]:
        return (self.observation, 0, False) #격자 바깥으로 나가지않기
```

### step(): 격자 색깔에 따른 처리
```py
cur_color = self.current_map[self.agent_state[0], self.agent_state[1]] # 현재 agent 색깔
new_color = self.current_map[next_state[0], next_state[1]] #새로운 색깔: 행동을 했을 때 색깔

if new_color == 0: # 새로운 색깔 ==0 빈칸
    if cur_color == 3: # 현재 색깔 ==3 agent --> agent가 빈칸으로 가는 형태
        self.current_map[self.agent_state[0], self.agent_state[1]] =0 # 원래 에이전트 =0 -<빈칸>
        self.current_map[next_state[0], next_state[1]] = 3    #에이전트가 옮겨간 위치
    self.agent_state = copy.deepcopy(next_state)

elif new_color == 1: #새로운 색상 == 1 == 장애물 == 이동 X
    return (self.observation, 0, False)

elif new_color == 2: # 2 == 도착지. 위치 서로 바꿈
    self.current_map[self.agent_state[0], self.agent_state[1]] = 0
    self.current_map[next_state[0], next_state[1]] = 4 #도착 완료상태
    self.agent_state = copy.deepcopy(next_state)
self.observation = self.gridmap_to_observation(self.current_map) #gridmap을 observation으로 넘김
if next_state[0] == self.target_state[0] and next_state[1] == self.target_state[1]:
    target_observation = copy.deepcopy(self.observation)
    return (target_observation, 1, True) # reward =1, 끝났다

else:
    return (self.observation, 0, False)

```

### reset & render
```py
def reset(self):
    self.agent_state = copy.deepcopy(self.start_state)
    self.current_map = copy.deepcopy(self.initial_map)
    self.observation = self.gridmap_to_observation(self.initial_map)
    return self.observation
#환경 초기화 

def render(self):
    img = self.observation
    plt.clf()
    plt.imshow(img)
    self.fig.canvas.show()
    plt.pause(0.00001)
#observation space를 rendering 해줌
```

### utility 함수 --> 파일로부터 격자 정보 읽어오기
```py
def read_grid_map(self, grid_map_path):
    with open(grid_map_path, 'r') as f:
        grid_map = f.readliness() # 읽기전용
    grids = np.array(list(map(lamda x: list(map(lambda y: int(y), x.split(''))), grid_map)))
# 파일에 있는 숫자를 받아 grid map으로 표현
    return grids

# 에이전트의 시작위치와 목표위치 정하기
def get_agent_states(self, initial_map):
    start_state = None
    target_state = None
    start_state = list(map(lambda x: x[0] if len(x)> 0 else None, np.where(initial_map ==2)))
    if start_state == [None, None] or target_state == [None, None]:
        sys.exit('start or target state not specified')
    return start_state, target_state

# 격자 맵을 관찰공간으로 변환
def gridmap_to_observation(self, grid_map, obs_shape=None):
    if obs_shape is None:
        obs_shape - self.obs_shape
    
    observation = np.zeros(obs_shape, dtype=np.float32)
    gs0 = int(observation.shape[0]/grid_map.shape[0])
    gs1 = int(observation.shape[1]/grid_map.shape[1])
    for i in range(grid_map.shape[1]):
        observation[i*gs0:(i+1)*gs0, j*gs1:(j+1)*gs1]= np.array(COLORS[grid_map[i, j]])
    return observation
```
![확인](https://user-images.githubusercontent.com/59559270/120480208-c253cc00-c3e9-11eb-8437-2304ab251136.PNG)
### OpenAI Gym 환경 등록
```py
from gym.envs.registration import register

register(
    id = 'gridworld-v0', 
    entry_point = 'gym_gridworld.envs:GridworldEnv',
)
```

### 강화학습 환경 잘 동작하는지 확인
```py
from __future__ import unicode_literals

import gym
import gym_gridworld

env = gym.make('gridworld-v0') # to make env

while True: #rendering 위해
    env.render()
        = env.step(env.action_space.sample()) # step 진행, 임의의 행동 진행
```

## case 1. baba-is-auto
![구조](https://user-images.githubusercontent.com/59559270/120480303-d7c8f600-c3e9-11eb-8731-5b0090ef21f9.PNG)
![환경구성](https://user-images.githubusercontent.com/59559270/120480306-d992b980-c3e9-11eb-9143-698447b33289.PNG)
```
state의 observability가 보장되지 못하는 환경을 POMDP(Partial Observability MDP)
```

## case 2. 하스스톤
![하스스톤구조](https://user-images.githubusercontent.com/59559270/120480313-dac3e680-c3e9-11eb-8c7f-af3a0f2de27d.PNG)
![추후](https://user-images.githubusercontent.com/59559270/120480320-dbf51380-c3e9-11eb-9e83-7708b62dd67a.PNG)
```
POMDP 환경

Monte Carlo Tree Search 알고리즘(MCTS)
    시뮬레이션을 통해 가장 승률이 좋은 행동을 하도록 하는 알고리즘
    1) 최대, 최소 값이 존재
    2) 게임 규칙의 존재
    3) 게임 길이 제한

```

# 강화학습
```
미지의 영역

Behind every great agent there's a great environment
복잡한 환경 만들기
    저장소에 확인

Reward를 주는 규칙에 대한 TIP
1. 공격적인 reward
2. 수비적인 reward
```