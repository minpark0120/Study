---
title: 2021-06-19-Sat-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------
# RL Gradient Descent
## 참고 https://brunch.co.kr/@chris-song/50

## Gradient Descent Optimization Algorithms
### Traditional Gradient Descent Algorithm
```
W=W−αdW
b=b−αdb
α: learning rate
```

### Momentum
```
Exponentially Weighted Averages
    새로운 데이터가 축적된 오래된 데이터들에게 큰 영향을 끼치지 못하도록 하는 방법
    β: momentum
    θ = new data
    V = Old_data * β + (1-β)θ 
    --> β가 클수록 영향 줄고, β가 작을수록 영향력 올라감
Bias Correction
    초반의 data들의 오차를 보정해주기 위해 기존 V에 (1-β^t)를 나누어준다.

기울기의 Exponentially Weighted Averages를 계산해 weight를 업데이트하는 방식
```

