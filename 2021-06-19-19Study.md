---
title: 2021-06-19-Sat-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------
# RL Gradient Descent
## 참고 https://brunch.co.kr/@chris-song/50

## Gradient Descent Optimization Algorithms
### Traditional Gradient Descent Algorithm
```
W=W−αdW
b=b−αdb
α: learning rate
```

### Momentum
```
Exponentially Weighted Averages
    새로운 데이터가 축적된 오래된 데이터들에게 큰 영향을 끼치지 못하도록 하는 방법
    β: momentum
    θ = new data
    V = Old_data * β + (1-β)θ 
    --> β가 클수록 영향 줄고, β가 작을수록 영향력 올라감
Bias Correction
    초반의 data들의 오차를 보정해주기 위해 기존 V에 (1-β^t)를 나누어준다.

기울기의 Exponentially Weighted Averages를 계산해 weight를 업데이트하는 방식

Gradient Descent with momentum
    Gradient Descent에서 기울기의 weighted Averages를 산출해 weight를 업데이트 
    Momentum을 사용함으로, 속도가 빠르고, SGD가 발산하거나 오버슈팅되는 것을 방지하며 local minimum 탈출이 가능하다.

    Learning Rate가 너무 작다면 학습속도가 느리고, 크다면 오버슈팅이나 발산의 위험성이 있다.

    최적값으로 가기 위해선 가로축으로 빠르게 전진해야하고 세로축으로는 천천히 진행해야 한다    
```
momentum1 사진
momentum2 사진

