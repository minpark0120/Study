---
title: 2021-06-16-Wen-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------
# RL Gradient Descent
## CNN
```
Abstract
    CNN은 input과 output간에 거리가 가까울수록 deeper, accruate, efficient. 
    위 논문에서는 위 항목들을 적용한 DenseNet을 소개한다. in a feed-forward fashion

    Traditional Convoultional Algorithm은 n개의 layer가 n개의 connection을 맺고있다. --> n^2
        반면에 DenseNet은 n(n+1)/2 
    
    DenseNet 장점
        Alleviate vanishing-gradient problem
        strengthen feature propagation
        encourage feature reuse
        reduce the number of parameters

Intro
    CNN이 deep해지면서 생긴 문제
        많은 layer을 지나가면서, 네트워크의 끝에 도달했을 때 사라졌을 수 있다.
        
        ResNet은  ID연결을 통해 한 레이어에서 다음 레이어로 신호를 우회한다
        
        Stochastic depth는 ResNet을 더 나은 gradient flow와 정보를 허용하기 위한 학습 중에 랜덤하게 layer들을 삭제함으로써 단축한다.

        FractalNets는 네트워크에서 많은 짧은 경로를 유지하면서 큰 nominal(명목) 깊이를 얻기 위해 여러 개의 병렬 계층 시퀀스를 서로 다른 수의 컨볼루션 블록과 반복적으로 결합합니다.

    --> they all share key characteristic

    feed-forward nature를 보존하기 위해 각 레이어는 이전의 모든 레이어에서 추가 입력을 얻고 자체 기능 맵에서 모든 후속 레이어로 전달합니다.

    ResNet과 다르게 우리는 절대 feature를 sum을 통해 combine하지 않고 concatenating(사슬같이 잇음)을 통해 combine 한다.


    *Fewer Parameters
    No need to relearn redundant feature-maps

    최근 Resnet 연구에 따르면 many layer는 관여율이 매우 적고 학습 도중 랜덤하게 drop될 수 있다.
        
        제안 된 DenseNet 아키텍처는 네트워크에 추가되는 정보와 보존되는 정보를 명시 적으로 구분합니다.

    DenseNets의 큰 장점 중 하나는 네트워크 전체에서 정보 흐름과 gradient가 개선되어 쉽게 학습 할 수 있다는 것입니다.

Conclusion
    DenseNet은 direct connections btw any two layers with the same fature-map size

    DenseNet은 성능 저하나 오버피팅등의 신호 없이 증가하는 매개변수로  정확도를 지속적으로 개선하려는 경향이 있다.

    DenseNet -> fewer parameters and less computation to acheive state of the art performance

    DenseNet은 자연스럽게 identity mapping과 deep supervision and diversified depth를 합침으로써 네트워크 전체에서 기능을 재사용할 수 있고 결과적으로 더 간결하게 학습할 수 있으며 실험에 따르면 더 정확한 모델을 학습할 수 있다.
```