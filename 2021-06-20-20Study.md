---
title: 2021-06-20-Sun-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------
# RL Gradient Descent
## 참고 https://brunch.co.kr/@chris-song/50
## 참고 https://velog.io/@minjung-s/Optimization-Algorithm#understanding-mini-batch-gradient-descent
## 참고 https://johnchenresearch.github.io/demon/


### RMSprop Algorithm
```
Root Mean Square Propatation
    기울기 강하의 속도를 증가시키는 알고리즘

    가로축의 속도는 dW로 조절하여 Sdw 조정
    세로축의 속도는 db로 조절하여 Sdb 조정
```
rms 사진

### Adam Optimization Algorithm
```
Adaptive Moment Estimation
    momentum과 RMSprop의 장점

초기화를 진행하고 Momentum과 RMSprop에서 사용한 v와 S 지정

Adam Optimization Algorithm
    initialize 후, v와 S 지정
α : learning rate
β : moment 
ϵ : constant
```
adam1
adam2
adam3

### AMSGrad
```
Adam은 optimal이 아닌 solution으로 수렴하는 설정에서 일부 mini-batch는 크고 유익한 기울기를 제공하지만, 이러한 mini-batch는 드물기 때문에 exponential averaging은 영향을 감소시켜 수렴이 좋지 않다. 

AMSGrad는 매개변수를 업데이트하기 위해 exponential average가 아닌 "과거 제곱 기울기"의 최대 값을 사용하는 새로운 알고리즘이다.

```
amsgrad1
```
Vt를 이용하는 대신 현재 것보다 큰 경우, Vt-1을 사용한다
```
amsgrad2
```
AMSGrad는 단계 크기를 증가시키지 않고 Adam이 겪는 문제 방지
단순화를 위해 Adam에서 보았던 debiasing step도 제거

```
amsgrad3
```
CIFAR-10에서만 좋은성능
나머진 bad
no One Really knows what amsgrad stands for
```
