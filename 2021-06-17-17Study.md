---
title: 2021-06-17-Thu-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------
# RL Gradient Descent
## CNN
### Related Work
```
    Cascade Structure로부터 fully connected multi layer perceptions - trained with batch gradient descent 
        --> effective on small datasets
    
    Utilizing multi-level features in CNNs through skip-connections --> effective for various vision tasks

    Highway Networks - effectively train end to end networks with more than 100 layers 
                         using bypassing paths, networks can be optimized without difficulty

    *bypassing path는 very deep networks에서 학습을 쉽게 해주는 중요한 factor이다. --> 추후 ResNet에 영향을 줌
        ResNet은 여러 기록을 갈아치운 알고리즘

    Stochastic depth - drop layers randomly 
        --> not all layers may be needed and highlights that there is a great amount of redundancy in deep networks 

    Orthogonal approach - to mincrease the network width

    Inception module(googlenet) - concatenates feature maps produced by filters of different sizes
        --> increasing the number of filters improves performance

    DenseNets는 학습하기 쉽거나 효율적인 응축된 모델을 yield하고 feature reuse를 통해 potential of the network를 exploit한다.

    Concatenating feature maps learned by different layers는 차후 들어오는 인풋의 variation을 증가시키고 효율을 증가시킨다.

    Incteption model과 비교했을 때, simpler and more efficient
```

### DenseNets
```
전통적인 convolutional feed-forward network는 l번째 layer의 아웃풋을 l+1 번째 layer의 인풋으로 사용한다
    ResNet은 skip-connection that bypasses tthe non-linear transformations with an identity function
        이 것의 장점은 gradient can flow directly throught the identity function from later layers to the earlier layers

Dense Connectivity
    Direct connections from any layer to all subsequent layers
        Xn = Hn([x1,x2,x3 ... xn-1]) --> concatenation of layers

    Composite function
        Batch Normalization: BN
        Rectified linear unit-ReLU
        3*3 convolution Conv


Pooling Layers
    concatenation은 feature maps의 사이즈가 변하면 실행되지 않는다
        convolutinal networks의 중요한 부분이 down-sampling이기 때문에 사이즈 변화는 피할 수없다.

    down-sampling을 위해 divide the network into multiple densely connected dense blocks

Growth rate
    regulates how much new info each layer contributes to the global state

    DenseNet can have very narrow layers
        small growth rate obtain 최첨단의(state of the art) results on the dataset
            each layer has access to all the preceding feature-maps --> to the network's collective knowledge
            no need to replicate it from layer to layer

Bottleneck layers
    input 조절하는 1*1 convolution acan be introduced as bottleneck layer before each 3*3 convolution 
        --> improves computational efficiency

Compression
    improve model compactness(빽빽함) -> reduce number of feature-maps
        m feature maps  -> e(m) output feature maps
            e = 1 --> number of feature maps unchanged
            e < 1 --> DenseNet-C -- set e = 0.5
            e < 1 + bottleneck --> DenseNet-BC

Implementation
    Three Dense blocks - equal number of layers
    before first dense block - convolution with 16 output channels
    


```