---
title: 2021-06-07-Mon-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------


# Paper Title: On the momentum term in gradient descent learning algorithms
```
1.	Problem

SGD를 사용하면 같은 학습률을 적용하기 때문에 Global한 최소값에 도달하지 못하고 local 최소값에 도달할 확률이 크다.

2.	Technical approaches

SGD가 가려는 방향으로 가속을 주는 방법이다. 보통 감마 값을 0.9로 설정해 가속한다.

3.	Major contribution

 Momentum 방식을 이용하여 local minimum을 빠져나오는 효과가 있다. 기존의 SGD를 이용하면 local minimum에 빠져 gradient가 0이 되어 이동할 수 없다.

4.	Main performance evaluation result(s)

 
 SGD가 local minimum에 갇혀 있을 때, momentum은 local min을 빠져나와 가고자 하는 방향으로 가속화되어 빠져나와 global min으로 수렴한다.

5.	Stochastic Critics and Comments 

 Momentum 방식은 local 최소값에서 벗어날 수 있다는 점에서 유의미하지만 가던 방향으로 가속하기 때문에 멈춰야 할 시점에도 관성에 의해 더 멀리 갈 수 있다.
```



# Paper Title: Nesterov Accelerated Gradient

```
1.	Problem

Momentum 방식은 가고자 하는 방향으로 가속하기 때문에 멈춰야 할 시점에도 관성에 의해 더 멀리 갈 수 있다.

2.	Technical approaches

 Momentum의 반정도를 이동한 후 어떠한 방식으로 이동할지 결정하는 방식이다. 즉, 관성을 조절하는 방식이다.

3.	Major contribution

 Local Minimum에 빠지지 않으면서 Momentum 방식의 빠른 이동에 대한 이점을 누리면서도, 멈춰야 하는 시점에 제동을 걸 수 있다는 점이 용이하다.

4.	Stochastic Critics and Comments 

 Momentum방식의 local min을 빠져나오는 가속화 방법을 채용하면서도, 멈춰야 할 시점에 제동을 거른 것이 유의미하다. 하지만 모든 파라미터에 대해 같은 learning rate를 적용한다는 것이 수렴하는 속도에 영향을 준다. 
```

