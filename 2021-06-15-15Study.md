---
title: 2021-06-15-Tue-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------
# RL Gradient Descent
## Adagrad
```
1.	Problem

기존 알고리즘들은 학습률이 너무 작으면 시간이 오래걸린다는 단점이 있었고 학습률이 너무 높으면 발산할 수 있기 때문에 학습이 실패할 수 있었다. 모든 파라미터에 같은 학습률을 적용해야 했다.
2.	Technical approaches

변수들을 update 할 때 각각 변수마다 step size를 다르게 설정해 이동하는 방식이다. 변화가 크다면 optimize 됬다고 생각해 learning rate를 줄이고, 변화가 작다면 optimize 되기 멀었다고 판단해 learning rate를 적게 줄인다

3.	Major contribution
 
 Adagrad는 매 단계마다 모든 매개 변수에 대해 다른 학습 속도를 사용하므로 Adagrad의 매개 변수 별 업데이트를 먼저 벡터화한다. 또한 학습 속도를 수동으로 조정할 필요가 없다

4.	Performance Evaluation

 ImageNet 실험에서는 AROW, PA, RDA 와 비교하였을 때 좋은 성과를 보였지만 큰 차이는 나지 않았고 Handwriting Recognition에서도 큰 차이를 보이지 않았다. Income Prediction에서는 앞선 두 실험에 비해선 좋은 결과를 보였다.
 

5.	Stochastic Critics and Comments

SGD에 비해서 사소한 오버헤드 계산 문제가 있다. 부족한 점으로는 학습을 계속해서 진행 시, step size가 작아져 거의 움직이지 않게 된다. 

```

## Adadelta
```
1.	Problem

 ADAGRAD는 학습이 오래 진행될수록 step size가 너무 작아져 움직이지 않게 된다.

2.	Technical approaches

 과거의 모든 제곱된 gradient를 축적하는 대신, 축적된 과거의 gradient의 창을 고정된 크기로 제한한다.
3.	Major contribution
 
  G를 구할 때 합을 구하는 대신 지수평균을 구하고, 여기에서는 step size를 단순하게 사용하는 대신 step size의 변화값의 제곱을 가지고 지수 평균 값을 사용한다.

4.	Performance Evaluation

 Handwritten Digit Classification에서는 SGD, Momentum, Adaagrad를 비교하였을 때, SGD와 momentum보단 성능이 좋았고, ADAGARD와 같이 initial convergence를 빠르게 이루었다. 한편 momentum과 같이 best performance에 수렴했다. 

5.	Stochastic Critics and Comments

Adadelta는 학습 스텝을 머신이 스스로 조절해 adagrad가 가진 학습스텝이 굉장이 낮아진다는 단점을 해결할 수 있다는 점에서 유의미한 논문이다.

```