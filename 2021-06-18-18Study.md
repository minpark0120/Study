---
title: 2021-06-18-Fri-Study
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------
# RL Gradient Descent
## 참고 https://brunch.co.kr/@chris-song/50

### 용어
```
1. J(θ): 목적함수
2. ∇θ: 기울기 
3. θ: 반대 방향으로 파라미터를 업데이트한 모델의 파라미터
4. 1*2: 목적함수의 기울기
5. η: learning rate -- 최소 값에 도달하기 위해 취하는 단계의 크기 결정
    --> 계곡에 도달할 때까지 목적 함수에 의해 생성 된 지표면의 기울기 방향을 따라 내리막 길을 따른다. 


```

## Gradient Descent Variants
```
목적 함수의 gradient를 계산하는 데 사용되는 데이터 양이 다름

데이터 양에 따라 매개 변수 업데이트의 정확성과 업데이트를 수행하는 데 걸리는 시간 사이 균형
```
### Batch Gradient Descent
```
Vanilla Gradeint Descent: 전체 트레이닝 데이터 세트에 대한 파라미터에 대한 비용 함수 그래디언트 계산

대규모 데이터 세트에 대한 중복 계산 수행
```

### Stochastic Gradient Descent
```
한 번에 하나의 업데이트만 수행

중복성 제거

훨씬 빨라 온라인 학습에 이용됨
```

### Mini-Batch Gradient Descent
```
매개 변수의 업데이트의 분산을 줄여 안정적인 수렴 유도

gradient computing을 가능하게하는 심층 학습 라이브러리에 공통으로 최적화 된 매트릭스 최적화를 사용할 수 있다.
```

```
Mini Batch Gradient Descent의 단점

적절한 학습 속도 선택 어려움. 학습 속도 너무 낮으면 느리게 수렴, 빠르면 수렴 X, Loss function이 최소 그 이상으로 변동됨

학습 속도 조정을 위해선 임계 값과 스케줄이 미리 정의되어야해 데이터 세트의 특성과 맞지 않음

동일한 학습 속도가 모든 매개 변수 업데이트에 적용. 하지만 데이터가 희박하고 기능의 빈도가 매우 다른경우 업데이트를 수행 안할 수도 있음

```