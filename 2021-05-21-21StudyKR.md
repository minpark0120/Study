---
title: 2021-05-21-Fri-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------


# RL
## DDPG
```
1.	Problem
DQN이 high-dimension observation space 문제를 해결했지만, discrete하거나 low-dimension action space에 국한된다는 문제점이 있었다.

2.	Technical approaches
DDPG는 Deep Q-Learning을 actor-critic을 활용해 deterministic policy를  학습하고 효과를 continuous action로 확장하였다. 

3.	Major contribution
DDPG는 높은 차원이면서 continuous한 action space의 policy를 학습할 수 있다. Replay buffer를 사용하여 data를 random하게 뽑아 correlation을 minimize하였다. Target Q-network를 생성해 학습 도중에 origin Q-network parameter를 가능한 고정시켜 학습이 converge할 수 있게 한다.


4.	Main performance evaluation result(s)
Cartpole이나 Torc라는 continuous 환경을 적용하여 low-dimension과 high-dimension에서 전부 잘 됨을 확인하였다. Target Network를 적용할 시 학습 수렴이 빨라지는 것을 확인하였다.

5.	Stochastic Critics and Comments 
Model-free 기반이기 때문에 학습 시간이 오래 걸린다. Continuous action space에 강화학습 알고리즘을 적용하였고 성능이 뛰어나기 때문에 자주 쓰인다.

```